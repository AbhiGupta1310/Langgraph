{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gemini Multimodal Example\n",
        "\n",
        "This notebook demonstrates how to use Google Gemini models for multimodal tasks (Text + Image) using LangChain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fitz  # PyMuPDF\n",
        "from langchain_core.documents import Document\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import torch\n",
        "import numpy as np\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.messages import HumanMessage\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "import base64\n",
        "import io\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "Loading weights: 100%|██████████| 398/398 [00:00<00:00, 1295.82it/s, Materializing param=visual_projection.weight]                                \n",
            "\u001b[1mCLIPModel LOAD REPORT\u001b[0m from: openai/clip-vit-base-patch32\n",
            "Key                                  | Status     |  | \n",
            "-------------------------------------+------------+--+-\n",
            "vision_model.embeddings.position_ids | UNEXPECTED |  | \n",
            "text_model.embeddings.position_ids   | UNEXPECTED |  | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CLIPModel(\n",
              "  (text_model): CLIPTextTransformer(\n",
              "    (embeddings): CLIPTextEmbeddings(\n",
              "      (token_embedding): Embedding(49408, 512)\n",
              "      (position_embedding): Embedding(77, 512)\n",
              "    )\n",
              "    (encoder): CLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): QuickGELUActivation()\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (vision_model): CLIPVisionTransformer(\n",
              "    (embeddings): CLIPVisionEmbeddings(\n",
              "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
              "      (position_embedding): Embedding(50, 768)\n",
              "    )\n",
              "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (encoder): CLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): QuickGELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
              "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv()\n",
        "\n",
        "# Set up the environment for Google\n",
        "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "### Initialize the CLIP Model for unified embeddings\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "### FIXED Embedding functions\n",
        "def embed_image(image_data):\n",
        "    \"\"\"Embed image using CLIP\"\"\"\n",
        "    if isinstance(image_data, str):  # If path\n",
        "        image = Image.open(image_data).convert(\"RGB\")\n",
        "    else:  # If PIL Image\n",
        "        image = image_data\n",
        "    \n",
        "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        vision_outputs = clip_model.vision_model(pixel_values=inputs['pixel_values'])\n",
        "        pooled = vision_outputs.pooler_output\n",
        "        features = clip_model.visual_projection(pooled)\n",
        "        features = features / features.norm(dim=-1, keepdim=True)\n",
        "        return features.squeeze().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "463ea88f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def embed_text(text):\n",
        "    \"\"\"Embed text using CLIP.\"\"\"\n",
        "    inputs = clip_processor(\n",
        "        text=text, \n",
        "        return_tensors=\"pt\", \n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=77\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        text_outputs = clip_model.text_model(\n",
        "            input_ids=inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask']\n",
        "        )\n",
        "        pooled = text_outputs.pooler_output\n",
        "        features = clip_model.text_projection(pooled)\n",
        "        features = features / features.norm(dim=-1, keepdim=True)\n",
        "        return features.squeeze().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Process PDF\n",
        "pdf_path = \"multimodal_sample.pdf\"\n",
        "doc = fitz.open(pdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Storage for all documents and embeddings\n",
        "all_docs = []\n",
        "all_embeddings = []\n",
        "image_data_store = {}  # Store actual image data for LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "62bc90cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text splitter\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "72e9553f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document('multimodal_sample.pdf')"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "e3d06ba1",
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, page in enumerate(doc):\n",
        "    ## Process text\n",
        "    text = page.get_text()\n",
        "    if text.strip():\n",
        "        # Create temporary document for splitting\n",
        "        temp_doc = Document(page_content=text, metadata={\"page\": i, \"type\": \"text\"})\n",
        "        text_chunks = splitter.split_documents([temp_doc])\n",
        "\n",
        "        # Embed each chunk using CLIP\n",
        "        for chunk in text_chunks:\n",
        "            embedding = embed_text(chunk.page_content)\n",
        "            all_embeddings.append(embedding)\n",
        "            all_docs.append(chunk)\n",
        "\n",
        "    ## Process images\n",
        "    for img_index, img in enumerate(page.get_images(full=True)):\n",
        "        try:\n",
        "            xref = img[0]\n",
        "            base_image = doc.extract_image(xref)\n",
        "            image_bytes = base_image[\"image\"]\n",
        "            \n",
        "            # Convert to PIL Image\n",
        "            pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "            \n",
        "            # Create unique identifier\n",
        "            image_id = f\"page_{i}_img_{img_index}\"\n",
        "            \n",
        "            # Store image as base64 for later use with Gemini\n",
        "            buffered = io.BytesIO()\n",
        "            pil_image.save(buffered, format=\"PNG\")\n",
        "            img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
        "            image_data_store[image_id] = img_base64\n",
        "            \n",
        "            # Embed image using CLIP\n",
        "            embedding = embed_image(pil_image)\n",
        "            all_embeddings.append(embedding)\n",
        "            \n",
        "            # Create document for image\n",
        "            image_doc = Document(\n",
        "                page_content=f\"[Image: {image_id}]\",\n",
        "                metadata={\"page\": i, \"type\": \"image\", \"image_id\": image_id}\n",
        "            )\n",
        "            all_docs.append(image_doc)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {img_index} on page {i}: {e}\")\n",
        "            continue\n",
        "\n",
        "doc.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "62d7a8f6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'page': 0, 'type': 'text'}, page_content='Annual Revenue Overview\\nThis document summarizes the revenue trends across Q1, Q2, and Q3. As illustrated in the chart\\nbelow, revenue grew steadily with the highest growth recorded in Q3.\\nQ1 showed a moderate increase in revenue as new product lines were introduced. Q2 outperformed\\nQ1 due to marketing campaigns. Q3 had exponential growth due to global expansion.'),\n",
              " Document(metadata={'page': 0, 'type': 'image', 'image_id': 'page_0_img_0'}, page_content='[Image: page_0_img_0]')]"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "2896324d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create unified FAISS vector store with CLIP embeddings\n",
        "embeddings_array = np.array(all_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "6699dd77",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.00267238,  0.01282993, -0.05183155, ..., -0.00385087,\n",
              "         0.02977716, -0.00010691],\n",
              "       [ 0.01732346, -0.01327705, -0.02427034, ...,  0.08993971,\n",
              "        -0.00272153,  0.03253055]], shape=(2, 512), dtype=float32)"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "d5bfcfae",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([Document(metadata={'page': 0, 'type': 'text'}, page_content='Annual Revenue Overview\\nThis document summarizes the revenue trends across Q1, Q2, and Q3. As illustrated in the chart\\nbelow, revenue grew steadily with the highest growth recorded in Q3.\\nQ1 showed a moderate increase in revenue as new product lines were introduced. Q2 outperformed\\nQ1 due to marketing campaigns. Q3 had exponential growth due to global expansion.'),\n",
              "  Document(metadata={'page': 0, 'type': 'image', 'image_id': 'page_0_img_0'}, page_content='[Image: page_0_img_0]')],\n",
              " array([[-0.00267238,  0.01282993, -0.05183155, ..., -0.00385087,\n",
              "          0.02977716, -0.00010691],\n",
              "        [ 0.01732346, -0.01327705, -0.02427034, ...,  0.08993971,\n",
              "         -0.00272153,  0.03253055]], shape=(2, 512), dtype=float32))"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(all_docs,embeddings_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "83dc1164",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        }
      ],
      "source": [
        "# Create custom FAISS index since we have precomputed embeddings\n",
        "vector_store = FAISS.from_embeddings(\n",
        "    text_embeddings=[(doc.page_content, emb) for doc, emb in zip(all_docs, embeddings_array)],\n",
        "    embedding=None,  # We're using precomputed embeddings\n",
        "    metadatas=[doc.metadata for doc in all_docs]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "3380c58a",
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "d064514d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_multimodal(query, k=5):\n",
        "    \"\"\"Unified retrieval using CLIP embeddings for both text and images.\"\"\"\n",
        "    # Embed query using CLIP\n",
        "    query_embedding = embed_text(query)\n",
        "    \n",
        "    # Search in unified vector store\n",
        "    results = vector_store.similarity_search_by_vector(\n",
        "        embedding=query_embedding,\n",
        "        k=k\n",
        "    )\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "0501a965",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_multimodal_message(query, retrieved_docs):\n",
        "    \"\"\"Create a message with both text and images for Gemini.\"\"\"\n",
        "    content = []\n",
        "    \n",
        "    # Add the query\n",
        "    content.append({\n",
        "        \"type\": \"text\",\n",
        "        \"text\": f\"Question: {query}\\n\\nContext:\\n\"\n",
        "    })\n",
        "    \n",
        "    # Separate text and image documents\n",
        "    text_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"text\"]\n",
        "    image_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"image\"]\n",
        "    \n",
        "    # Add text context\n",
        "    if text_docs:\n",
        "        text_context = \"\\n\\n\".join([\n",
        "            f\"[Page {doc.metadata['page']}]: {doc.page_content}\"\n",
        "            for doc in text_docs\n",
        "        ])\n",
        "        content.append({\n",
        "            \"type\": \"text\",\n",
        "            \"text\": f\"Text excerpts:\\n{text_context}\\n\"\n",
        "        })\n",
        "    \n",
        "    # Add images\n",
        "    for doc in image_docs:\n",
        "        image_id = doc.metadata.get(\"image_id\")\n",
        "        if image_id and image_id in image_data_store:\n",
        "            content.append({\n",
        "                \"type\": \"text\",\n",
        "                \"text\": f\"\\n[Image from page {doc.metadata['page']}]:\\n\"\n",
        "            })\n",
        "            content.append({\n",
        "                \"type\": \"image_url\",\n",
        "                \"image_url\": {\n",
        "                    \"url\": f\"data:image/png;base64,{image_data_store[image_id]}\"\n",
        "                }\n",
        "            })\n",
        "    \n",
        "    # Add instruction\n",
        "    content.append({\n",
        "        \"type\": \"text\",\n",
        "        \"text\": \"\\n\\nPlease answer the question based on the provided text and images.\"\n",
        "    })\n",
        "    \n",
        "    return HumanMessage(content=content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "3900abee",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query: What does the chart on page 1 show about revenue trends?\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Retrieved 2 documents:\n",
            "  - Text from page 0: Annual Revenue Overview\n",
            "This document summarizes the revenue trends across Q1, Q2, and Q3. As illust...\n",
            "  - Image from page 0\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Answer:**\n",
              "\n",
              "The chart shows that revenue grew steadily across Q1, Q2, and Q3, with the highest growth recorded in Q3. The blue bar represents Q1, the green bar represents Q2, and the red bar represents Q3. The height of the bars indicates revenue, with the red bar (Q3) being the tallest, followed by the green bar (Q2), and then the blue bar (Q1). This visual representation confirms the text's description of steady growth and highest growth in Q3."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "\n",
            "Query: Summarize the main findings from the document\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Retrieved 2 documents:\n",
            "  - Text from page 0: Annual Revenue Overview\n",
            "This document summarizes the revenue trends across Q1, Q2, and Q3. As illust...\n",
            "  - Image from page 0\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Answer:**\n",
              "\n",
              "The document summarizes revenue trends across Q1, Q2, and Q3, indicating steady growth with the highest growth in Q3. Q1 saw a moderate increase due to new product introductions. Q2 outperformed Q1, attributed to marketing campaigns. Q3 experienced exponential growth driven by global expansion."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "\n",
            "Query: What visual elements are present in the document?\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Retrieved 2 documents:\n",
            "  - Text from page 0: Annual Revenue Overview\n",
            "This document summarizes the revenue trends across Q1, Q2, and Q3. As illust...\n",
            "  - Image from page 0\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Answer:**\n",
              "\n",
              "The document contains a bar chart. The bar chart has three bars of different heights and colors: blue, green, and red. The text mentions that the document summarizes revenue trends across Q1, Q2, and Q3, and that the chart illustrates this. The text also states that revenue grew steadily, with the highest growth in Q3. This suggests that the bars in the chart represent revenue for Q1, Q2, and Q3, with the blue bar likely representing Q1, the green bar representing Q2, and the red bar representing Q3, given their increasing heights."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "def multimodal_pdf_rag_pipeline(query):\n",
        "    \"\"\"Main pipeline for multimodal RAG with Gemini.\"\"\"\n",
        "    # Retrieve relevant documents\n",
        "    context_docs = retrieve_multimodal(query, k=5)\n",
        "    \n",
        "    # Create multimodal message\n",
        "    message = create_multimodal_message(query, context_docs)\n",
        "    \n",
        "    # Get response from Gemini\n",
        "    response = llm.invoke([message])\n",
        "    \n",
        "    # Print retrieved context info\n",
        "    print(f\"\\nRetrieved {len(context_docs)} documents:\")\n",
        "    for doc in context_docs:\n",
        "        doc_type = doc.metadata.get(\"type\", \"unknown\")\n",
        "        page = doc.metadata.get(\"page\", \"?\")\n",
        "        if doc_type == \"text\":\n",
        "            preview = doc.page_content[:100] + \"...\" if len(doc.page_content) > 100 else doc.page_content\n",
        "            print(f\"  - Text from page {page}: {preview}\")\n",
        "        else:\n",
        "            print(f\"  - Image from page {page}\")\n",
        "    print(\"\\n\")\n",
        "    \n",
        "    return response.content\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "if __name__ == \"__main__\":\n",
        "    # Example queries\n",
        "    queries = [\n",
        "        \"What does the chart on page 1 show about revenue trends?\",\n",
        "        \"Summarize the main findings from the document\",\n",
        "        \"What visual elements are present in the document?\"\n",
        "    ]\n",
        "    \n",
        "    for query in queries:\n",
        "        print(f\"\\nQuery: {query}\")\n",
        "        print(\"-\" * 70)\n",
        "        answer = multimodal_pdf_rag_pipeline(query)\n",
        "        \n",
        "        # Display the answer with Markdown formatting\n",
        "        display(Markdown(f\"**Answer:**\\n\\n{answer}\"))\n",
        "        print(\"=\" * 70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
